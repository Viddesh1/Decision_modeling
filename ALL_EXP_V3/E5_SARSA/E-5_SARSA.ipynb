{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym version:  0.24.0\n",
      "Numpy Version:  1.23.5\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries for implementing SARSA learning.\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import pickle\n",
    "\n",
    "print(\"Gym version: \", gym.__version__)\n",
    "print(\"Numpy Version: \", np.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Building the environment\n",
    "Here, we will be using the ‘FrozenLake-v0’ environment which is preloaded into gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the environment\n",
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Initializing different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining the different parameters\n",
    "epsilon = 0.9\n",
    "total_episodes = 10000\n",
    "max_steps = 100\n",
    "alpha = 0.85\n",
    "gamma = 0.95\n",
    "\n",
    "#Initializing the Q-matrix\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Q # printing Q-table\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Defining utility functions to be used in the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to choose the next action\n",
    "def choose_action(state):\n",
    "\taction=0\n",
    "\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\taction = env.action_space.sample()\n",
    "\telse:\n",
    "\t\taction = np.argmax(Q[state, :])\n",
    "\treturn action\n",
    "\n",
    "#Function to learn the Q-value\n",
    "def update(state, state2, reward, action, action2):\n",
    "\tpredict = Q[state, action]\n",
    "\ttarget = reward + gamma * Q[state2, action2]\n",
    "\tQ[state, action] = Q[state, action] + alpha * (target - predict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Training the learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the reward\n",
    "reward=0\n",
    "\n",
    "# Starting the SARSA learning\n",
    "for episode in range(total_episodes):\n",
    "\tt = 0\n",
    "\tstate1 = env.reset()\n",
    "\taction1 = choose_action(state1)\n",
    "\n",
    "\twhile t < max_steps:\n",
    "\t\t#Visualizing the training\n",
    "\t\tenv.render()\n",
    "\t\t\n",
    "\t\t#Getting the next state\n",
    "\t\tstate2, reward, done, info = env.step(action1)\n",
    "\n",
    "\t\t#Choosing the next action\n",
    "\t\taction2 = choose_action(state2)\n",
    "\t\t\n",
    "\t\t#Learning the Q-value\n",
    "\t\tupdate(state1, state2, reward, action1, action2)\n",
    "\n",
    "\t\tstate1 = state2\n",
    "\t\taction1 = action2\n",
    "\t\t\n",
    "\t\t#Updating the respective vaLues\n",
    "\t\tt += 1\n",
    "\t\treward += 1\n",
    "\t\t\n",
    "\t\t#If at the end of learning process\n",
    "\t\tif done:\n",
    "\t\t\tbreak\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluating the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the performance\n",
    "print (\"Performance : \", reward/total_episodes)\n",
    "\n",
    "#Visualizing the Q-matrix\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the Q-table in pickle file for easy loading when needed.\n",
    "\n",
    "with open(\"frozenLake_qTable.pkl\", 'wb') as f:\n",
    "    pickle.dump(Q, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Table Representation\n",
    "\n",
    "# Q = [[0.61328675 0.58403178 0.59475622 0.60545697]\n",
    "#  [0.56521894 0.11293677 0.50370929 0.60326189]\n",
    "#  [0.59057488 0.58249108 0.57499097 0.59089953]\n",
    "#  [0.57255264 0.01738344 0.57441336 0.59935962]\n",
    "#  [0.62387633 0.5546865  0.71629256 0.11403143]\n",
    "#  [0.         0.         0.         0.        ]\n",
    "#  [0.64606929 0.71730524 0.73581135 0.11986364]\n",
    "#  [0.         0.         0.         0.        ]\n",
    "#  [0.61203901 0.70121817 0.69993882 0.79860727]\n",
    "#  [0.1464226  0.87045385 0.16544007 0.13804508]\n",
    "#  [0.91957935 0.91595476 0.57216828 0.68562543]\n",
    "#  [0.         0.         0.         0.        ]\n",
    "#  [0.         0.         0.         0.        ]\n",
    "#  [0.81536073 0.03436849 0.87831349 0.70812791]\n",
    "#  [0.95772482 0.88879209 0.92707794 0.99582979]\n",
    "#  [0.         0.         0.         0.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time \n",
    "import pickle\n",
    "import gym\n",
    "import os\n",
    "\n",
    "env = gym.make('FrozenLake-v1') # Create an instance of the FrozenLake-v1 environment\n",
    "\n",
    "# Un-pickling the pickle file using load() function. To convert from pickle to python object.\n",
    "with open(\"frozenLake_qTable.pkl\", 'rb') as f: \n",
    "\tQ = pickle.load(f)\n",
    "\n",
    "# Choosing the maximum value from the state and action\n",
    "def choose_action(state):\n",
    "\taction = np.argmax(Q[state, :])\n",
    "\treturn action\n",
    "\n",
    "# Rendering the environment and choosing the best action based on policy\n",
    "for episode in range(5):\n",
    "\n",
    "\tstate = env.reset()\n",
    "\tprint(\"*** Episode: \", episode)\n",
    "\tt = 0\n",
    "\twhile t < 100:\n",
    "\t\tenv.render()\n",
    "\n",
    "\t\taction = choose_action(state)  \n",
    "\t\t\n",
    "\t\tstate2, reward, done, info = env.step(action)  \n",
    "\t\t\n",
    "\t\tstate = state2\n",
    "\n",
    "\t\tif done:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\ttime.sleep(0.5)\n",
    "\t\tos.system('clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='SARSA_Formula.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 2.43272530e+00]\n",
      " [1.90508246e+00 5.82769817e-04]\n",
      " [1.38758459e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "Optimal policy:\n",
      "State 0: Action 1\n",
      "State 1: Action 0\n",
      "State 2: Action 0\n",
      "State 3: Action 0\n",
      "State 4: Action 0\n"
     ]
    }
   ],
   "source": [
    "# SARSA Algorithm Implementation\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the environment\n",
    "n_states = 5\n",
    "n_actions = 2\n",
    "reward_table = np.array([[0, 1], [0, 0], [0, 0], [0, 0], [1, 0]])  # Rewards for each state-action pair\n",
    "\n",
    "# Define the hyperparameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "# Initialize the Q-table\n",
    "q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "# Define the SARSA algorithm\n",
    "def sarsa(state, q_table, epsilon):\n",
    "    # Choose the action using an epsilon-greedy policy\n",
    "    if random.random() < epsilon:\n",
    "        action = random.randint(0, n_actions - 1)\n",
    "    else:\n",
    "        action = np.argmax(q_table[state])\n",
    "    \n",
    "    # Take the chosen action and observe the new state and reward\n",
    "    new_state = state + (2 * action - 1)  # Move left or right\n",
    "    if new_state < 0 or new_state >= n_states:\n",
    "        return state, action  # If the new state is invalid, return the current state and action\n",
    "    reward = reward_table[state, action]\n",
    "    \n",
    "    # Choose the next action using an epsilon-greedy policy\n",
    "    if random.random() < epsilon:\n",
    "        next_action = random.randint(0, n_actions - 1)\n",
    "    else:\n",
    "        next_action = np.argmax(q_table[new_state])\n",
    "    \n",
    "    # Update the Q-table using the SARSA rule\n",
    "    q_table[state, action] = q_table[state, action] + learning_rate * \\\n",
    "                             (reward + discount_factor * q_table[new_state, next_action] - q_table[state, action])\n",
    "    \n",
    "    return new_state, next_action\n",
    "\n",
    "# Run the SARSA algorithm\n",
    "state = 2  # Initial state\n",
    "for i in range(100):\n",
    "    state, _ = sarsa(state, q_table, epsilon)\n",
    "\n",
    "print(q_table, end=\"\\n\\n\")\n",
    "\n",
    "# Obtain the optimal policy from the Q-table\n",
    "optimal_policy = np.argmax(q_table, axis=1)\n",
    "\n",
    "# Print the optimal policy\n",
    "print(\"Optimal policy:\")\n",
    "for s in range(n_states):\n",
    "    print(f\"State {s}: Action {optimal_policy[s]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
